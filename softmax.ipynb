{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "\n",
    "@torch.jit.script\n",
    "def naive_softmax(x):\n",
    "    \"\"\"Compute row-wise softmax of X using native pytorch\n",
    "\n",
    "    We subtract the maximum element in order to avoid overflows. Softmax is invariant to\n",
    "    this shift.\n",
    "    \"\"\"\n",
    "    # read  MN elements ; write M  elements\n",
    "    x_max = x.max(dim=1)[0]\n",
    "    # read MN + M elements ; write MN elements\n",
    "    z = x - x_max[:, None]\n",
    "    # read  MN elements ; write MN elements\n",
    "    numerator = torch.exp(z)\n",
    "    # read  MN elements ; write M  elements\n",
    "    denominator = numerator.sum(dim=1)\n",
    "    # read MN + M elements ; write MN elements\n",
    "    ret = numerator / denominator[:, None]\n",
    "    # in total: read 5MN + 2M elements ; wrote 3MN + 2M elements\n",
    "    return ret\n",
    "\n",
    "@triton.jit\n",
    "def softmax_kernel(output_ptr, input_ptr, input_row_stride, output_row_stride, n_cols, BLOCK_SIZE: tl.constexpr):\n",
    "    # The rows of the softmax are independent, so we parallelize across those\n",
    "    row_idx = tl.program_id(0)\n",
    "    # The stride represents how much we need to increase the pointer to advance 1 row\n",
    "    row_start_ptr = input_ptr + row_idx * input_row_stride\n",
    "    # The block size is the next power of two greater than n_cols, so we can fit each\n",
    "    # row in a single block\n",
    "    col_offsets = tl.arange(0, BLOCK_SIZE)\n",
    "    input_ptrs = row_start_ptr + col_offsets\n",
    "    # Load the row into SRAM, using a mask since BLOCK_SIZE may be > than n_cols\n",
    "    row = tl.load(input_ptrs, mask=col_offsets < n_cols, other=-float('inf'))\n",
    "    # Subtract maximum for numerical stability\n",
    "    row_minus_max = row - tl.max(row, axis=0)\n",
    "    # Note that exponentiation in Triton is fast but approximate (i.e., think __expf in CUDA)\n",
    "    numerator = tl.exp(row_minus_max)\n",
    "    denominator = tl.sum(numerator, axis=0)\n",
    "    softmax_output = numerator / denominator\n",
    "    # Write back output to DRAM\n",
    "    output_row_start_ptr = output_ptr + row_idx * output_row_stride\n",
    "    output_ptrs = output_row_start_ptr + col_offsets\n",
    "    tl.store(output_ptrs, softmax_output, mask=col_offsets < n_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
